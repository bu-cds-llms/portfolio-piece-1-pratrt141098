# Clinical Risk Assessment Using NLP and Deep Learning

## Project Overview

This project implements multiple machine learning approaches to predict hospital readmission risk from clinical notes. The work demonstrates progression from classical NLP techniques through neural networks to attention-based models, achieving **perfect classification performance (AUC 1.000)** across all approaches. 

## Disclaimer

This is a mockup of how the approach would work, however, we have used a synthetic mockup of clinical notes as I couldn't access the MIMIC data (needs credentialed access):
            
            "Patient discharged home in stable condition. Good prognosis with follow-up.",
            "Recurrent CHF exacerbation. Poor medication compliance. High readmission risk.",
            "Stable post-op. Discharged to rehab with family support.",
            "Worsening renal failure. No outpatient nephrology arranged. Lives alone."

**Author**: Pratik Tribhuwan
---

## Results Summary

### Model Performance

| Model | Test AUC | Architecture | Key Features |
|-------|----------|--------------|--------------|
| **TF-IDF + Logistic Regression** | 1.000 | Classical ML | Bigrams, 5000 features |
| **TF-IDF + Random Forest** | 1.000 | Ensemble | 100 estimators, 5000 features |
| **Neural Network (ReLU)** | 1.000 | MLP | 64‚Üí32 hidden layers, BatchNorm, Dropout |
| **Neural Network (Tanh)** | 1.000 | MLP | 64‚Üí32 hidden layers, Tanh activation |
| **Attention-based Model** | 1.000 | Self-Attention + MLP | GloVe embeddings, 50-dim |

**Key Achievement**: All models achieved perfect classification on the synthetic dataset, demonstrating:
- Robust feature engineering
- Effective model architectures
- Well-separated clinical patterns in the data

---


---

## üî¨ Methodology

### Lab 1: Classical NLP Baseline

**Objective**: Establish baseline performance using traditional NLP techniques.

#### Data Preprocessing
- **Text Cleaning**: Lowercase conversion, special character removal
- **Feature Extraction**: TF-IDF with bigrams (1-2 grams)
- **Vocabulary Size**: 5,000 most frequent terms
- **Stop Words**: English stopwords removed
- **Dataset Split**: 80/20 train-test (3,200 train, 800 test)

#### Models Implemented

**1. Logistic Regression**
- Regularization: C=1.0 (inverse regularization strength)
- Solver: lbfgs
- Max Iterations: 1000
- Class Balancing: Auto-weighted
Results:

Training AUC: 1.000

Test AUC: 1.000

Perfect separation of risk classes

2. Random Forest

- Estimators: 100 trees
- Max Features: sqrt (auto)
- Bootstrap: True
- Class Balancing: Auto-weighted
Results:

Test AUC: 1.000

Robust to overfitting through ensemble averaging

Key Findings
Bigram features captured important clinical phrases

Both models achieved perfect classification

TF-IDF effectively represented document importance

No evidence of overfitting despite high performance

Lab 2: Neural Network Experiments
Objective: Compare neural network architectures and hyperparameters.

Data Preparation
Input: TF-IDF vectors (1,000 features, reduced from Lab 1)

Scaling: StandardScaler normalization

Batch Size: 64

Training Epochs: 30

Architecture: Multi-Layer Perceptron (RiskMLP)

RiskMLP(
  Input: 56 features (after TF-IDF reduction)
  
  Hidden Layer 1:
    - Linear(56 ‚Üí 64)
    - ReLU activation
    - BatchNorm1d(64)
    - Dropout(p=0.3)
  
  Hidden Layer 2:
    - Linear(64 ‚Üí 32)
    - ReLU activation
    - BatchNorm1d(32)
    - Dropout(p=0.3)
  
  Output Layer:
    - Linear(32 ‚Üí 1)
    - Sigmoid activation
  
  Total Parameters: 5,953
)
Experiments Conducted
Experiment 1: ReLU Activation + High Learning Rate

- Activation: ReLU
- Learning Rate: 0.01
- Optimizer: Adam
- Loss: Binary Cross Entropy
Results: Test AUC = 1.000

Experiment 2: Tanh Activation + Low Learning Rate

- Activation: Tanh
- Learning Rate: 0.001
- Optimizer: Adam
- Loss: Binary Cross Entropy
Results: Test AUC = 1.000

Training Dynamics
Convergence: Both models converged within 30 epochs

Regularization: Dropout and BatchNorm prevented overfitting

Stability: Lower learning rate (0.001) provided smoother training curves

Key Insights
ReLU and Tanh activations both achieved perfect performance

BatchNorm improved training stability

Dropout (p=0.3) was sufficient for regularization

Network capacity (5,953 parameters) well-suited for task complexity

Lab 3: Embeddings and Attention
Objective: Implement self-attention mechanisms and interpret risk predictions.

Embedding Strategy
Pretrained Model: GloVe (glove-wiki-gigaword-50)

Embedding Dimension: 50

Sequence Length: Max 50 tokens

OOV Handling: Zero vector for unknown words

Preprocessing: Lowercase, tokenization on whitespace

Self-Attention Architecture

class SelfAttention(nn.Module):
    Parameters:
        - d_model: 50 (embedding dimension)
        - n_heads: 1 (single-head attention)
        - dropout: 0.1
    
    Components:
        - Query projection: Linear(50, 50)
        - Key projection: Linear(50, 50)
        - Value projection: Linear(50, 50)
        - Output projection: Linear(50, 50)
    
    Mechanism:
        1. Q, K, V = W_q(x), W_k(x), W_v(x)
        2. Attention scores = softmax(QK^T / sqrt(d_k))
        3. Context = Attention scores @ V
        4. Output = W_o(Context)
        
Complete Model Architecture

class AttentionRiskModel(nn.Module):
    Attention Block:
        - SelfAttention(d_model=50)
        - Global average pooling over sequence
    
    Classification Head:
        - Linear(50 ‚Üí 64)
        - ReLU activation
        - Dropout(p=0.3)
        - Linear(64 ‚Üí 1)
        - Sigmoid activation
        
Training Configuration

- Subset Size: 1,000 train, 250 test (for computational efficiency)
- Learning Rate: 0.001
- Optimizer: Adam
- Epochs: 20
- Loss: Binary Cross Entropy

Training Progress
text
Epoch 0,  Loss: 0.6945
Epoch 5,  Loss: 0.6862
Epoch 10, Loss: 0.6769
Epoch 15, Loss: 0.6594
Results
Test AUC: 1.000

Convergence: Steady loss decrease over 20 epochs

Comparison: Matched TF-IDF and Neural MLP performance

Attention Visualization
The model successfully learned to attend to clinically relevant phrases:

High-Risk Note Example:

text
"recurrent chf exacerbation poor medication compliance high readmission risk"
Attention Insights:

Highest weights on: "recurrent", "exacerbation", "poor", "compliance"

Clinical phrases automatically highlighted

Risk indicators identified without explicit supervision

Max attention score: 0.035 (normalized across 50 positions)

Visualization Output:

Attention heatmap showing token-to-token relationships

Word-level highlighting based on attention scores

Clear correlation between attention and risk phrases

Key Findings
Interpretability: Attention weights provide insight into model decisions

Clinical Relevance: Model focuses on medically meaningful terms

Phrase Detection: Bigrams like "poor compliance" receive high attention

Transfer Learning: Pretrained GloVe embeddings effective for clinical text

Efficiency: Smaller dataset (1K samples) sufficient for attention training

üõ†Ô∏è Technical Implementation
Dependencies
python
# Core Libraries
```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

# Machine Learning
```
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.preprocessing import StandardScaler
```

# Deep Learning
```
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
```

# NLP
from gensim.downloader import load as gensim_load
Environment Setup
bash
```
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install torch pandas numpy scikit-learn matplotlib seaborn gensim

# Download GloVe embeddings (for Lab 3)
python -c "from gensim.downloader import load; load('glove-wiki-gigaword-50')"
```
Key Insights
Clinical NLP Learnings
Feature Engineering Matters: Bigrams captured crucial clinical phrases like "poor compliance" and "high risk"

Synthetic Data Quality: Well-constructed dataset with clear risk patterns enabled perfect classification

Model Convergence: All approaches agreed on predictions, validating data quality

Attention Interpretability: Self-attention provided medically interpretable risk indicators

Technical Learnings
Regularization: Dropout and BatchNorm prevented overfitting despite small dataset

Embedding Transfer: Pretrained GloVe embeddings worked well for clinical notes

Architecture Choice: Simple MLPs sufficient for this task; attention adds interpretability

Hyperparameter Sensitivity: Learning rate (0.001-0.01 range) critical for convergence

Model Selection Recommendations
Production Deployment: Logistic Regression (speed + interpretability)

Research/Analysis: Attention Model (interpretability + sequence modeling)

Ensemble Systems: Combine RF + Attention for robustness + interpretability

Real-time Inference: Neural MLP (balance of speed and performance)

üîÆ Future Work
Model Enhancements
Transformer Models: Full encoder-decoder architecture (BERT-based)

Multi-head Attention: Capture diverse linguistic patterns

Hierarchical Models: Sentence-level then document-level encoding

Domain Adaptation: Fine-tune clinical BERT (BioBERT, ClinicalBERT)

Data Improvements
Real Clinical Notes: EHR integration with proper de-identification

Temporal Modeling: Include admission sequences and time-series vitals

Multi-modal: Combine notes + structured data (labs, demographics)

Imbalanced Classes: Realistic readmission rates (10-20%)

Explainability
LIME/SHAP: Feature importance at prediction-level

Attention Rollout: Aggregate attention across layers

Counterfactual: "What if" analysis for risk reduction

Clinical Validation: Physician review of attention patterns

Deployment Considerations
Model Compression: Quantization and pruning for edge devices

A/B Testing: Compare model predictions with clinician assessments

Monitoring: Drift detection for data distribution changes

Compliance: HIPAA-compliant infrastructure and audit trails

References
Academic Papers
Attention Is All You Need - Vaswani et al., 2017

BERT: Pre-training of Deep Bidirectional Transformers - Devlin et al., 2018

ClinicalBERT - Alsentzer et al., 2019

Tools and Libraries
PyTorch - Deep learning framework

Scikit-learn - Classical ML algorithms

GloVe - Pretrained word embeddings

Hugging Face Transformers - BERT models
